#
# Copyright 2016 The Regents of The University California
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""
This script generates graphs of number of threads per disk vs. JCT. The data is assumed to have been
generated by the run_basic_disk_job.py script.
"""

import argparse
import math
from matplotlib import pyplot
from matplotlib.backends import backend_pdf
from os import path

import parse_event_logs


def main():
  args = __parse_args()
  warmup_count = args.warmup_count
  spark_jcts = __get_jcts_from_logs(args.spark_log_dir, warmup_count)
  monotasks_1_jcts = __get_jcts_from_logs(args.monotasks_1_log_dir, warmup_count)
  monotasks_4_jcts = __get_jcts_from_logs(args.monotasks_4_log_dir, warmup_count)
  monotasks_6_jcts = __get_jcts_from_logs(args.monotasks_6_log_dir, warmup_count)
  __create_num_threads_vs_jct_graph(spark_jcts, monotasks_1_jcts, monotasks_4_jcts,
    monotasks_6_jcts, args.output_dir, phase="write")
  __create_num_threads_vs_jct_graph(spark_jcts, monotasks_1_jcts, monotasks_4_jcts,
    monotasks_6_jcts, args.output_dir, phase="read")


def __parse_args():
  parser = argparse.ArgumentParser(description="")
  parser.add_argument(
    "-s",
    "--spark-log-dir",
    help="Logs from a run of the experiment using Spark.",
    required=True)
  parser.add_argument(
    "-m1",
    "--monotasks-1-log-dir",
    help="Logs from a run of the experiment using Monotasks with 1 thread per disk.",
    required=True)
  parser.add_argument(
    "-m4",
    "--monotasks-4-log-dir",
    help="Logs from a run of the experiment using Monotasks with 4 threads per disk.",
    required=True)
  parser.add_argument(
    "-m6",
    "--monotasks-6-log-dir",
    help="Logs from a run of the experiment using Monotasks with 6 threads per disk.",
    required=True)
  parser.add_argument(
    "-o",
    "--output-dir",
    help="The directory in which to store the graph PDFs. Defaults to the value of --log-dir.",
    required=True)
  parser.add_argument(
    "-w",
    "--warmup-count",
    default=0,
    help="The number of iterations that are warmup and should be discarded.",
    required=True,
    type=int)

  args = parser.parse_args()

  spark_log_dir = args.spark_log_dir
  assert path.isdir(spark_log_dir), \
    "The supplied log directory for Spark does not exist or is a file: %s" % spark_log_dir
  monotasks_1_log_dir = args.monotasks_1_log_dir
  assert path.isdir(monotasks_1_log_dir), ("The supplied log directory for Monotasks with 1 " +
    "thread per disk does not exist or is a file: %s" % monotasks_1_log_dir)
  monotasks_4_log_dir = args.monotasks_4_log_dir
  assert path.isdir(monotasks_4_log_dir), ("The supplied log directory for Monotasks with 4 " +
    "threads per disk does not exist or is a file: %s" % monotasks_4_log_dir)
  monotasks_6_log_dir = args.monotasks_6_log_dir
  assert path.isdir(monotasks_6_log_dir), ("The supplied log directory for Monotasks with 6 " +
    "threads per disk does not exist or is a file: %s" % monotasks_6_log_dir)

  return args


def __get_jcts_from_logs(log_dir, warmup_count):
  """
  Returns a tuple of (list of write job JCTs, list of read job JCTs) parsed from the event log
  contained in the provided directory.
  """
  event_log_filepath = path.join(log_dir, "event_log")
  sorted_job_pairs = sorted(parse_event_logs.Analyzer(event_log_filepath).jobs.iteritems())
  return (__get_jcts_for_phase(sorted_job_pairs, warmup_count, phase="write"),
    __get_jcts_for_phase(sorted_job_pairs, warmup_count, phase="read"))


def __get_jcts_for_phase(sorted_job_pairs, warmup_count, phase):
  """
  Returns a list of JCTs (in seconds) for the specified phase. sorted_job_pairs should be a list of
  the form [ ( job ID, job ) ], sorted by job ID. warmup_count indicates the number of jobs that
  should be discarded from the beginning of each phase. Assumes that the jobs correspond to one run
  of org.apache.spark.examples.monotasks.disk.DiskThroughputExperiment.
  """
  assert phase in ["write", "read"]
  filterer = __write_job_filterer if phase == "write" else __read_job_filterer
  return [job.runtime() / 1000.0 for job in filterer(warmup_count, sorted_job_pairs)]


def __create_num_threads_vs_jct_graph(
    spark_jcts,
    monotasks_1_jcts,
    monotasks_4_jcts,
    monotasks_6_jcts,
    output_dir,
    phase):
  assert phase in ["write", "read"]

  jcts_list = [spark_jcts, monotasks_1_jcts, monotasks_4_jcts, monotasks_6_jcts]

  num_ticks = 5
  xmax = num_ticks - 1
  max_jct = max([jct
    for write_jcts, read_jcts in jcts_list
    for jct in (write_jcts if phase == "write" else read_jcts)])
  ymax = max_jct * 1.1
  pyplot.title("Comparing Spark and Monotasks using SSDs (%s phase)" % phase)
  pyplot.ylabel("JCT (s)")
  pyplot.grid(b=True)
  pyplot.xlim(xmin=0, xmax=xmax)
  pyplot.ylim(ymin=0, ymax=ymax)

  # Build a list of lists of JCTs, sorted by num threads per disk.
  all_jcts = [write_jcts if phase == "write" else read_jcts for write_jcts, read_jcts in jcts_list]
  pyplot.boxplot(all_jcts, whis=[0, 100])

  # Replace the visually-correct x-axis values with the numerically correct values.
  pyplot.xticks(xrange(num_ticks), ["", "Spark", "Monotasks 1", "Monotasks 4", "Monotasks 6", ""])

  # Save the graph as a PDF.
  output_filepath = path.join(output_dir, "%s_phase_spark_vs_monotasks_ssds.pdf" % phase)
  with backend_pdf.PdfPages(output_filepath) as pdf:
    pdf.savefig()

  pyplot.close()


def __write_job_filterer(warmup_count, sorted_job_pairs):
  """
  The first job generates the data. Of the remaining jobs, the first half are writes. warmup_count
  specifies the number of jobs that are warmup and should be discarded.
  """
  max_write_job_id = math.ceil(len(sorted_job_pairs) / 2.0) - 1
  return [job for (job_id, job) in sorted_job_pairs
    if (job_id > warmup_count) and (job_id <= max_write_job_id)]


def __read_job_filterer(warmup_count, sorted_job_pairs):
  """
  The first job generates the data. Of the remaining jobs, the second half are reads. warmup_count
  specifies the number of jobs that are warmup and should be discarded.
  """
  min_read_job_id = math.ceil(len(sorted_job_pairs) / 2.0) + warmup_count
  return [job for (job_id, job) in sorted_job_pairs if job_id >= min_read_job_id]


if __name__ == "__main__":
  main()
